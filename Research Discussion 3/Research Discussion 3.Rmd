---
title: "Data 612 - Research Discussion 3"
author: "Paul Perez"
date: "7/14/2020"
output:
  html_document:
    df_print: paged
    highlight: pygments
    theme: yeti
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  pdf_document:
    toc: yes
---

\newpage

# Recommendation Systems, Predictive Analytics - Pitfalls of Algorithmic Discrimination

## Instructions
As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments.

## Response
In general, with the access to more and more data, algorithms and predictive analytics will continue to learn more and increase in precision, with respect to the goal of each situation. We focus primarily on collaborative based filter recommender systems in the class and examine how we can use the relation of users to recommend items, as well as look at the relationship among different items to further recommend items. Content based recommender systems will use the descriptive content of items to recommend similar items. There are numerous ways to normalize and prep the data, but the ultimate goal is to use the most appropriate system to make a recommendation. The issue likes in reinforcing human bias, as generating recommendations for a specific subset of user's who share a common demographic feature. If the recommender isn't representative of the full population, a bias can easily be generated from a subset of the population. Its fairly common to see on Facebook, that each user profile has qualitative data that is generated by facebook, such as political view, based on the content users interact with. This is an area where classifying users can lead to recommended content that may fall to a bias. This can lead to persuasion of views for users. With the growth of bots and fake accounts, social media networks are left to curb this issue of spreading any false information and imposing views based on the generated qualities of other users. For example, Twitter tries to remove the content and or avoid the spread of content from some accounts that raise red flags of being bots.

\newpage

### Sources
MLconf SEA 2016. (2016, May 27). When Recommendations Systems Go Bad - Evan Estola (MeetUp) [Video]. YouTube. https://www.youtube.com/watch?v=MqoRzNhrTnQ

Equality of Opportunity in Supervised Learning
https://arxiv.org/pdf/1610.02413.pdf